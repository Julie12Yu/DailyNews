Day 3:

What is a Naive Bayes Classifier? (Baby edition)

The Naive Bayes Classifier focuses on probabilities, notably the probability of a situation occuring depending on factors present within the dataset. 
It's very statistics-y, utilizing the Bayes Theorem, where h is the probability of an occurrence, and D is the probability of a situation. 

Looks something like: P(h|D) = (P(D|H)*P(h))/(P(D))

Because the classifier is naive, it assumes that the factors are not interdependent, even if the factors are interdependent. 
The classifier calculates each probability and puts them into the formula!

The Naive Bayes Classifier is especially good because it is simple (Bayes Theorem!), fast, and can work on larger data sets!
However, the base assumption of Naive Bayes (that factors are independent) is not true. 
Furthermore, if P(h) or P(D) are ever zero, we run into the Zero Frequency Problem.
There seem to be 3 ways to correct:

Laplace correction: add 1 to every count

Lindstone correction: Add small constant to every count

Absolute discounting: subtract a constant




Read:
https://www.datacamp.com/tutorial/naive-bayes-scikit-learn



Will read:
https://medium.com/@dancerworld60/demystifying-na%C3%AFve-bayes-log-probability-and-laplace-smoothing-d6da61b0e70b
