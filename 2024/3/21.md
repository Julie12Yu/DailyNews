Day 4:

Naive Bayes pt 2
===

We'll get into the ways to deal with issues with Naive Bayes tomorrow! For today, we'll take a look at the types of Naive Bayes.
There are 5 main types which serve different purposes. 

1. Gaussian Naive Bayes:
Used when all the features are numerical.
Assumes all features follow a Gaussian distribution.
Gaussian distribution is then used to find the probability of the event occurring.
If the distribution of the dataset is different, but is also a well-known distribution, the formula for that distribution is used to find the probability.
It is worth noting that the Gaussian distribution does encounter the Zero Frequency Problem (Gaussian distribution == no 0).

2. Categorical Naive Bayes:
Used when features are categorical, i.e., colors or types of objects. 
Assumes all features follow a categorical distribution.
Used to find probability of event occurring in categorical situations (i.e., types of weather). 
It is worth noting that Laplace smoothing does not apply to the denominator within the Bayes equation.

3. Bernoulli Naive Bayes:
Used when features are binary (Yes/No, 1/0, True/False).
Use-cases are similar to Multinomial Naive Bayes. Works well on smaller datasets, and classifying text.
Can be used in the case of marking whether words occur (another test adding onto the Multinomial test).
It is worth noting that Laplace smoothing can occur here? Unclear :c

4. Multinomial Naive Bayes:
Used to classify texts, used when dataset has discrete (discontinuous and definite) features and have a multinomial (k version of binomial) distribution.
Assumes features are frequencies of words in text.
Classifies whether a document is a certain type (i.e., of spam or not) based on frequencies of words present.
It is worth noting that Laplace smoothing is used here. 

5. Complement Naive Bayes:
Used when dataset is not balanced, AKA when the distribution is not uniform (compensates for Multinomial Naive Bayes).
Instead of finding probability of an item belonging to one class, we calculate probability for item to belong to all classes.
This is the complement. We basically calculate the opposite and return it - if we are calculating the probability of an item belonging to a class,
we find the probability of that item not belonging to all the classes, and we select the lowest probability. The complement! 
Outperforms Gaussian and Multinomial Naive Bayes on text classification tasks. 


READ:

https://medium.com/@dancerworld60/demystifying-na%C3%AFve-bayes-log-probability-and-laplace-smoothing-d6da61b0e70b 

https://www.geeksforgeeks.org/multinomial-naive-bayes/

https://scikit-learn.org/stable/modules/naive_bayes.html#:~:text=1.9.4.-,Bernoulli%20Naive%20Bayes,(Bernoulli%2C%20boolean)%20variable.
