Day 2:

What is a Variational Autoencoder (review/part 1)

We've learned the textbook definition of lossy compression in CSE 123 at UW - 
Variational Autoencoders (VAE) seems to build on the idea of compressing and decompressing to generate new images or data. 
Unlike the Autoencoder (AE) which focuses on compressing then decompressing (efficiently maintaining the old image/dataset), 
the VAE can be said to focus on generating the 'average' image within the provided data.

There is a key difference between the AE and VAE, which is how they treat/view the data within the latent space.
With the AE, the data is taken in with the goal of being able to be decoded into it's original form.
Thus, the relationships found within the latent space of the AE (especially when overfitted) may be nonsense.
Furthermore, the AE does not generate new content.

However, whilst using a VAE, the data is taken with the goal of being able to find a 'distribution' to it, or see it's 'pattern' (kind of, I think T^T).
Within the latent space, instead of simply making things as tight as possible, a distribution (IE Gaussian/Normal) will be fitted onto the latent space,
thus creating a unique output.
